{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab3d2a76-9d46-41e5-9706-0d23daeeb33d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gina Wang\n",
    "# Dr. Morales & Nana\n",
    "# QMSSGR5069 Applied Data Sciences - Take Home Exercise #2\n",
    "# March 24, 2025\n",
    "# Collaborator: Jay Jun (We went through the assignment together and talked about ways to approach each question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "648c0d13-b852-480a-947e-a66bd7bc3877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Question 1: [20 pts] Build any model of your choice with tunable hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5864a5a0-9bd6-4c09-8527-485cc5aea106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages to read from S3\n",
    "%pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "125adac4-fdde-4924-92cb-c2488cbabf43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Restart the Python kernel to activate s3fs\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "515af2bc-3f54-4c1b-9f1b-fba68320c4a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "037d284a-21d4-4430-92ad-88c026c1be4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load datasets from S3\n",
    "results = pd.read_csv(\"s3://columbia-gr5069-main/raw/results.csv\")\n",
    "races = pd.read_csv(\"s3://columbia-gr5069-main/raw/races.csv\")\n",
    "drivers = pd.read_csv(\"s3://columbia-gr5069-main/raw/drivers.csv\")\n",
    "lap_times = pd.read_csv(\"s3://columbia-gr5069-main/raw/lap_times.csv\")\n",
    "pit_stops = pd.read_csv(\"s3://columbia-gr5069-main/raw/pit_stops.csv\")\n",
    "qualifying = pd.read_csv(\"s3://columbia-gr5069-main/raw/qualifying.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b083784-3e14-47ac-bdc5-df4785e6e978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join: results + races + drivers\n",
    "results_merged = results.merge(races, on=\"raceId\", suffixes=(\"\", \"_race\"))\n",
    "results_merged = results_merged.merge(drivers, on=\"driverId\", suffixes=(\"\", \"_driver\"))\n",
    "\n",
    "# Feature: average lap time per driver per race\n",
    "lap_avg = lap_times.groupby(['raceId', 'driverId'])['milliseconds'].mean().reset_index()\n",
    "lap_avg.rename(columns={'milliseconds': 'avg_lap_time_ms'}, inplace=True)\n",
    "\n",
    "# Feature: number of pit stops per driver per race\n",
    "pit_count = pit_stops.groupby(['raceId', 'driverId']).size().reset_index(name='num_pit_stops')\n",
    "\n",
    "# Feature: qualifying position (lowest value if multiple attempts)\n",
    "qualifying_agg = qualifying.groupby(['raceId', 'driverId'])['position'].min().reset_index()\n",
    "qualifying_agg.rename(columns={'position': 'qualifying_position'}, inplace=True)\n",
    "\n",
    "# Merge engineered features\n",
    "results_merged = results_merged.merge(lap_avg, on=['raceId', 'driverId'], how='left')\n",
    "results_merged = results_merged.merge(pit_count, on=['raceId', 'driverId'], how='left')\n",
    "results_merged = results_merged.merge(qualifying_agg, on=['raceId', 'driverId'], how='left')\n",
    "\n",
    "# Select features for modeling\n",
    "model_data = results_merged[[\n",
    "    'raceId', 'driverId', 'grid', 'positionOrder', 'points',\n",
    "    'avg_lap_time_ms', 'num_pit_stops', 'qualifying_position'\n",
    "]]\n",
    "\n",
    "# Drop rows with missing data\n",
    "model_data = model_data.dropna(subset=[\n",
    "    'positionOrder', 'avg_lap_time_ms', 'num_pit_stops', 'qualifying_position'\n",
    "])\n",
    "\n",
    "# Display to verify\n",
    "display(model_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22802ea-fef0-41aa-bd3c-8c7f74da6cb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Question 2: [20 pts] Create an experiment setup where - for each run - you log: the hyperparameters used in the model, the model itself, every possible metric from the model you chose, at least two artifacts (plots, or csv files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6906d84b-5e33-4734-a745-9b9cd16ca41b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean and prep for ML\n",
    "df = model_data.copy()\n",
    "\n",
    "# Drop non-numeric columns if needed\n",
    "df = df.select_dtypes(include=[np.number])  # Drop all non-numeric cols\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=[\"positionOrder\"])\n",
    "y = df[\"positionOrder\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Train model and log with MLflow\n",
    "with mlflow.start_run(run_name=\"Basic RF Experiment\") as run:\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(X_train, y_train)\n",
    "    predictions = rf.predict(X_test)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(rf, \"random-forest-model\")\n",
    "\n",
    "    # Metrics\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    print(f\"  mse: {mse}\")\n",
    "\n",
    "    # Log metadata\n",
    "    runID = run.info.run_uuid\n",
    "    experimentID = run.info.experiment_id\n",
    "    print(\"Inside MLflow Run with run_id {} and experiment_id {}\".format(runID, experimentID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "addf1099-390a-4f65-9ee9-e53f66dac539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Question 3: [20 pts] Track your MLFlow experiment and run at least 10 experiments with different parameters each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "033c9fe0-df93-4f62-86e2-9767ba24117b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define function to train, evaluate, and log\n",
    "def log_rf_run(experimentID, run_name, params, X_train, X_test, y_train, y_test):\n",
    "    with mlflow.start_run(experiment_id=experimentID, run_name=run_name) as run:\n",
    "        rf = RandomForestRegressor(**params)\n",
    "        rf.fit(X_train, y_train)\n",
    "        predictions = rf.predict(X_test)\n",
    "\n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(rf, \"rf-model\")\n",
    "\n",
    "        # Log hyperparameters\n",
    "        for param, value in params.items():\n",
    "            mlflow.log_param(param, value)\n",
    "\n",
    "        # Metrics\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"mse\", mse)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "        print(f\"[{run_name}] MSE: {mse:.2f} | MAE: {mae:.2f} | R²: {r2:.3f}\")\n",
    "\n",
    "        # Feature importance\n",
    "        importance = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': rf.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "        temp_csv = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
    "        importance.to_csv(temp_csv.name, index=False)\n",
    "        mlflow.log_artifact(temp_csv.name, \"feature-importance\")\n",
    "\n",
    "        # Residual plot\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.residplot(x=predictions, y=y_test, lowess=True, ax=ax)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.title(\"Model Residual Plot\")\n",
    "\n",
    "        temp_img = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")\n",
    "        fig.savefig(temp_img.name)\n",
    "        mlflow.log_artifact(temp_img.name, \"residual-plot\")\n",
    "\n",
    "        return run.info.run_uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf5119b6-13d0-4ceb-83ef-48bfb8d2f007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Run 10 experiments with different parameters\n",
    "params_list = [\n",
    "    {\"n_estimators\": 120, \"max_depth\": 5, \"random_state\": 42},\n",
    "    {\"n_estimators\": 240, \"max_depth\": 5, \"random_state\": 42},\n",
    "    {\"n_estimators\": 360, \"max_depth\": 5, \"random_state\": 42},\n",
    "    {\"n_estimators\": 120, \"max_depth\": 10, \"random_state\": 42},\n",
    "    {\"n_estimators\": 240, \"max_depth\": 10, \"random_state\": 42},\n",
    "    {\"n_estimators\": 360, \"max_depth\": 10, \"random_state\": 42},\n",
    "    {\"n_estimators\": 120, \"max_depth\": 15, \"random_state\": 42},\n",
    "    {\"n_estimators\": 240, \"max_depth\": 15, \"random_state\": 42},\n",
    "    {\"n_estimators\": 360, \"max_depth\": 15, \"random_state\": 42},\n",
    "    {\"n_estimators\": 500, \"max_depth\": 20, \"random_state\": 42},\n",
    "]\n",
    "\n",
    "for i, params in enumerate(params_list, 1):\n",
    "    run_name = f\"Run {i}: {params['n_estimators']} Estimators, Depth {params['max_depth']}\"\n",
    "    log_rf_run(experimentID, run_name, params, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0688154f-4c37-4e4f-9fce-9a562829dd93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Question 4: [20 pts] Select your best model run and explain why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1888957d-75cc-4847-b397-5f46d3d82345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After carefully evaluating the performance metrics from all ten Random Forest model runs, I selected Run 5 - which used 240 estimators and a maximum tree depth of 10 - as the best model. This choice is grounded in a close examination of the three primary evaluation metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), and R² score.\n",
    "\n",
    "Run 5 produced an MSE of 3.96, MAE of 1.16, and an R² value of 0.889. What’s notable is that these values are identical or nearly identical to several other runs (specifically Runs 6, 8, and 9), but Run 5 achieves this level of performance with a smaller number of trees and moderate tree depth, which makes it both computationally more efficient and likely less prone to overfitting. For instance, Run 6 used 360 estimators with the same depth of 10 and achieved the same MSE and R², but at a greater computational cost. Similarly, Run 8 required a deeper tree (depth 15) and also 240 estimators to match the MSE and R² of Run 5, introducing more complexity into the model without delivering better performance.\n",
    "\n",
    "While all models reported the same MAE of 1.16 - which suggests that on average, the absolute error per prediction was consistent across the board - the marginal gains in R² and MSE plateaued after a certain level of complexity. Run 5 hit the sweet spot in that trade-off. It reflects a balance between bias and variance: it's deep and wide enough to capture the signal in the data, but not so complex that it risks overfitting to noise. From a model selection perspective, parsimony, or choosing the simplest model that performs well, is a valuable guiding principle, and Run 5 aligns well with this.\n",
    "\n",
    "Therefore, although several models yielded nearly identical predictive accuracy, Run 5 offers the most efficient path to strong performance, which makes it the most optimal choice from both a statistical and practical standpoint. In a real-world deployment setting, this kind of model would likely generalize better while also consuming fewer computational resources during training and inference. \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Take_Home_Exercise_#3_GinaWang",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
